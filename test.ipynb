{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88ee163a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY=os.getenv(\"GOOGLE_API_KEY\")\n",
    "TAVILY_API_KEY=os.getenv(\"TAVILY_API_KEY\")\n",
    "GROQ_API_KEY=os.getenv(\"GROQ_API_KEY\")\n",
    "LANGSMITH_API_KEY=os.getenv(\"LANGSMITH_API_KEY\")\n",
    "LANGSMITH_PROJECT=os.getenv(\"LANGSMITH_PROJECT\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "os.environ[\"TAVILY_API_KEY\"] = TAVILY_API_KEY\n",
    "os.environ[\"GROQ_API_KEY\"]= GROQ_API_KEY\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = LANGSMITH_API_KEY\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"]=LANGSMITH_PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7f1b961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'langgraph-prerequist'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LANGSMITH_PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02840817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\langgraph-end-to-end\\langgraphvenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/embedding-gecko-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/gemini-2.5-pro-exp-03-25\n",
      "models/gemini-2.5-pro-preview-03-25\n",
      "models/gemini-2.5-flash-preview-04-17\n",
      "models/gemini-2.5-flash-preview-05-20\n",
      "models/gemini-2.5-flash-preview-04-17-thinking\n",
      "models/gemini-2.5-pro-preview-05-06\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-preview-image-generation\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/gemini-2.5-flash-preview-tts\n",
      "models/gemini-2.5-pro-preview-tts\n",
      "models/learnlm-2.0-flash-experimental\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n",
      "models/gemma-3n-e4b-it\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/aqa\n",
      "models/imagen-3.0-generate-002\n",
      "models/veo-2.0-generate-001\n",
      "models/gemini-2.5-flash-preview-native-audio-dialog\n",
      "models/gemini-2.5-flash-preview-native-audio-dialog-rai-v3\n",
      "models/gemini-2.5-flash-exp-native-audio-thinking-dialog\n",
      "models/gemini-2.0-flash-live-001\n"
     ]
    }
   ],
   "source": [
    "from google.generativeai import list_models\n",
    "import os\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "models = list_models()\n",
    "for model in models:\n",
    "    print(model.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c924125",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\langgraph-end-to-end\\langgraphvenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff224a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi there! How can I help you today?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"hi\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36f9e41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")'''\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "llm=ChatGroq(model_name=\"Gemma2-9b-It\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e30a8af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're looking at Ohm's Law, which states:\n",
      "\n",
      "* **V = I * R**\n",
      "\n",
      "Where:\n",
      "\n",
      "* **V** is **voltage** (measured in volts) - the electrical potential difference that pushes electrons through a circuit.\n",
      "* **I** is **current** (measured in amperes) - the flow of electrons through a conductor.\n",
      "* **R** is **resistance** (measured in ohms) - the opposition to the flow of current.\n",
      "\n",
      "**Resistance (R) is the key part of your question.**\n",
      "\n",
      "Resistance is like friction for electricity.  Just as friction slows down a moving object, resistance slows down the flow of electrons.\n",
      "\n",
      "Here's what resistance means in Ohm's Law:\n",
      "\n",
      "* **High Resistance:**  A material with high resistance offers a lot of opposition to current flow. Think of a narrow pipe; it's harder for water to flow through than a wide pipe.\n",
      "* **Low Resistance:** A material with low resistance allows current to flow easily. Think of a wide pipe; water flows through it with less resistance.\n",
      "\n",
      "**Factors Affecting Resistance:**\n",
      "\n",
      "* **Material:** Different materials have different inherent resistances. For example, copper is a good conductor with low resistance, while rubber is an insulator with high resistance.\n",
      "* **Length:**  The longer a conductor, the higher its resistance.\n",
      "* **Cross-sectional Area:** The thicker (larger cross-sectional area) a conductor, the lower its resistance.\n",
      "* **Temperature:**  In most materials, resistance increases with temperature.\n",
      "\n",
      "\n",
      "Let me know if you'd like more details about any of these aspects!\n",
      "\n",
      "goodbye take care yourself\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    question=input(\"type your question. if you want to quit the chat write quit\")\n",
    "    if question !=\"quit\":\n",
    "        print(llm.invoke(question).content)\n",
    "    else:\n",
    "        print(\"goodbye take care yourself\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c7b1c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33a2a77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the LLM with your OpenAI API key (make sure OPENAI_API_KEY is set in your environment)\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")  # or \"gpt-4o\", \"gpt-4\", etc.\n",
    "\n",
    "# Call the LLM with a prompt\n",
    "response = llm.invoke(\"What is the capital of France?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08396728",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
